{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ðŸ‘‹ This code is heavily inspired from [Sebastian Raschka](https://x.com/rasbt)'s excellent book, \"[Build a Large Language Model (From Scratch)](https://www.manning.com/books/build-a-large-language-model-from-scratch)\".\n",
        "\n",
        "I highly recommend buying and reading that book.\n",
        "\n",
        "My code here is taken directly from Sebastian's book, but with some slight modifications to make it financial domain-specific.\n",
        "\n",
        "Questions?  Feel free to DM me on [Twitter](https://x.com/virattt)."
      ],
      "metadata": {
        "id": "IUfUT14mxuBj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Setup and installation"
      ],
      "metadata": {
        "id": "u2ler1zErC0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets tiktoken --quiet"
      ],
      "metadata": {
        "id": "vrIwtxyorB3m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5677b2a-b60a-40a6-e631-c0482d6c6a50"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m \u001b[32m471.0/480.6 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/179.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow>=2.15.0  tqdm>=4.66 --quiet"
      ],
      "metadata": {
        "id": "_hP3kWFZHj5v"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "url = (\n",
        "    \"https://raw.githubusercontent.com/rasbt/\"\n",
        "    \"LLMs-from-scratch/main/ch05/\"\n",
        "    \"01_main-chapter-code/gpt_download.py\"\n",
        ")\n",
        "filename = url.split('/')[-1]\n",
        "urllib.request.urlretrieve(url, filename)"
      ],
      "metadata": {
        "id": "MxO0zcYXHgIP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee59de8b-97ad-4e32-bdd5-d15dc3a93116"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('gpt_download.py', <http.client.HTTPMessage at 0x78f1103ee290>)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Download instruction dataset"
      ],
      "metadata": {
        "id": "t6qgbnGzxxgl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, DatasetDict, load_dataset, concatenate_datasets\n",
        "\n",
        "# Pull the existing dataset from huggingface\n",
        "dataset_path_hf = \"virattt/financial-qa-10K\"\n",
        "existing_dataset_hf = load_dataset(dataset_path_hf)"
      ],
      "metadata": {
        "id": "TZjM6mEpq4wY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming existing_dataset_hf is your DatasetDict\n",
        "train_dataset = existing_dataset_hf['train']\n",
        "\n",
        "# Convert to a list of dictionaries\n",
        "dataset_list = train_dataset.to_list()\n",
        "\n",
        "# Extract question, answer, and context\n",
        "data = [{'instruction': \"Answer the following question.\", 'input': item['question'], 'output': item['answer']} for item in dataset_list]"
      ],
      "metadata": {
        "id": "SjPkF4yMrJ5Q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Example entry:\\n\", data[500])"
      ],
      "metadata": {
        "id": "LmbiHYQkxvcm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b808424e-3568-4ed4-c132-78b4d145d79b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example entry:\n",
            " {'instruction': 'Answer the following question.', 'input': 'When did Costco Wholesale Corporation begin its operations?', 'output': 'Costco Wholesale Corporation began its operations in 1983.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Another example entry:\\n\", data[1000])"
      ],
      "metadata": {
        "id": "tlNgto3Ox14E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac0ff3fd-5b57-43e6-8956-0d85f4f8fb60"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Another example entry:\n",
            " {'instruction': 'Answer the following question.', 'input': 'What year was American Express founded and when was it incorporated as a New York corporation?', 'output': 'American Express was founded in 1850 and was incorporated as a New York corporation in 1965.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Implement prompt formatter"
      ],
      "metadata": {
        "id": "4MHDbwIGx4X1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_input(entry):\n",
        "    instruction_text = (\n",
        "        f\"Below is an instruction that describes a task. \"\n",
        "        f\"Write a response that appropriately completes the request.\"\n",
        "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
        "    )\n",
        "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
        "    return instruction_text + input_text"
      ],
      "metadata": {
        "id": "SCZ93qQho9cH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_input = format_input(data[500])\n",
        "desired_response = f\"\\n\\n### Response:\\n{data[500]['output']}\"\n",
        "print(model_input + desired_response)"
      ],
      "metadata": {
        "id": "iOAywH-MpQcM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "827ac3b8-5ce1-4811-c307-c28ff6bd2897"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Answer the following question.\n",
            "\n",
            "### Input:\n",
            "When did Costco Wholesale Corporation begin its operations?\n",
            "\n",
            "### Response:\n",
            "Costco Wholesale Corporation began its operations in 1983.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Partition the dataset"
      ],
      "metadata": {
        "id": "00NKjbRLpBTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_portion = int(len(data) * 0.85)  # 85% for training\n",
        "test_portion = int(len(data) * 0.1)   # 10% for testing\n",
        "val_portion = len(data) - train_portion - test_portion  # Remaining 5% for validation\n",
        "\n",
        "train_data = data[:train_portion]\n",
        "test_data = data[train_portion:train_portion + test_portion]\n",
        "val_data = data[train_portion + test_portion:]\n",
        "\n",
        "print(\"Training set length:\", len(train_data))\n",
        "print(\"Validation set length:\", len(val_data))\n",
        "print(\"Test set length:\", len(test_data))"
      ],
      "metadata": {
        "id": "JPoyYgJqpKVm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f87aeeb-9bdd-4fab-9f10-6b942a7d71ff"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set length: 5950\n",
            "Validation set length: 350\n",
            "Test set length: 700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Implement InstructionDataset class"
      ],
      "metadata": {
        "id": "32XwXZzTpMQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class InstructionDataset(Dataset):\n",
        "  def __init__(self, data, tokenizer):\n",
        "    self.data = data\n",
        "    self.encoded_texts = []\n",
        "\n",
        "    for entry in data:\n",
        "      instruction_plus_input = format_input(entry)\n",
        "      response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
        "      full_text = instruction_plus_input + response_text\n",
        "      self.encoded_texts.append(\n",
        "          tokenizer.encode(full_text)\n",
        "      )\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.encoded_texts[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)"
      ],
      "metadata": {
        "id": "0XIepNT19Ea7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyMNyKcB9c7p",
        "outputId": "5d871872-7ef8-4c34-8c98-a726c2cf915a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[50256]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Implement custom dataset collate function"
      ],
      "metadata": {
        "id": "WnMu-3eYpmV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(\n",
        "    batch,\n",
        "    pad_token_id=50256,\n",
        "    ignore_index=-100,\n",
        "    allowed_max_length=None,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    batch_max_length = max(len(item)+1 for item in batch)\n",
        "    inputs_lst, targets_lst = [], []\n",
        "\n",
        "    for item in batch:\n",
        "        new_item = item.copy()\n",
        "        new_item += [pad_token_id]\n",
        "        # Pad sequences to max_length\n",
        "        padded = new_item + [pad_token_id] * (batch_max_length - len(new_item))\n",
        "        inputs = torch.tensor(padded[:-1])  # Truncate the last token for inputs\n",
        "        targets = torch.tensor(padded[1:])  # Shift +1 to the right for targets\n",
        "\n",
        "        mask = targets == pad_token_id\n",
        "        indices = torch.nonzero(mask).squeeze()\n",
        "        if indices.numel() > 1:\n",
        "            targets[indices[1:]] = ignore_index\n",
        "\n",
        "        if allowed_max_length is not None:\n",
        "            inputs = inputs[:allowed_max_length]\n",
        "            targets = targets[:allowed_max_length]\n",
        "\n",
        "        inputs_lst.append(inputs)\n",
        "        targets_lst.append(targets)\n",
        "\n",
        "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
        "    targets_tensor = torch.stack(targets_lst).to(device)\n",
        "    return inputs_tensor, targets_tensor"
      ],
      "metadata": {
        "id": "f_AWjcgfqEJS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test out the function\n",
        "inputs_1 = [0, 1, 2, 3, 4]\n",
        "inputs_2 = [5, 6]\n",
        "inputs_3 = [7, 8, 9]\n",
        "batch = (\n",
        "    inputs_1,\n",
        "    inputs_2,\n",
        "    inputs_3\n",
        ")\n",
        "inputs, targets = custom_collate_fn(batch)\n",
        "print(inputs)\n",
        "print(targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YGfOQbeq3XI",
        "outputId": "60b39ef5-9214-4926-9fbc-7abbd1e53ed1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[    0,     1,     2,     3,     4],\n",
            "        [    5,     6, 50256, 50256, 50256],\n",
            "        [    7,     8,     9, 50256, 50256]])\n",
            "tensor([[    1,     2,     3,     4, 50256],\n",
            "        [    6, 50256,  -100,  -100,  -100],\n",
            "        [    8,     9, 50256,  -100,  -100]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "id": "F-zX-wyvru6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0099bfd0-b2d7-40ea-84d4-ef68845cb741"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import partial\n",
        "customized_collate_fn = partial(custom_collate_fn, device=device, allowed_max_length=1024)"
      ],
      "metadata": {
        "id": "kvSsaxc8tI1j"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Initialize data loaders"
      ],
      "metadata": {
        "id": "gDjploPxtTgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "num_workers = 0\n",
        "batch_size = 8\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_dataset = InstructionDataset(train_data, tokenizer)\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "val_dataset = InstructionDataset(val_data, tokenizer)\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")\n",
        "\n",
        "test_dataset = InstructionDataset(test_data, tokenizer)\n",
        "test_loader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    collate_fn=customized_collate_fn,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=num_workers\n",
        ")"
      ],
      "metadata": {
        "id": "z8O7x7mOtXTC"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Implement GPTModel"
      ],
      "metadata": {
        "id": "hXAvOR41FGdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self, emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps = 1e-5\n",
        "    self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim=-1, keepdim=True)\n",
        "    var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "    normalized_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "    return self.scale * normalized_x + self.shift"
      ],
      "metadata": {
        "id": "_8d9jxwXFLJv"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))"
      ],
      "metadata": {
        "id": "CVjyqN4dFZyg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement feed-forward neural network\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Linear(config[\"emb_dim\"], 4 * config[\"emb_dim\"]),\n",
        "        GELU(),\n",
        "        nn.Linear(4 * config[\"emb_dim\"], config[\"emb_dim\"]),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)"
      ],
      "metadata": {
        "id": "0L9t5v9xFa9v"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "\n",
        "    assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "\n",
        "    self.d_out = d_out                  # 768\n",
        "    self.num_heads = num_heads          # 12\n",
        "    self.head_dim = d_out // num_heads  # 64\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.out_proj = nn.Linear(d_out, d_out)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer(\n",
        "        'mask',\n",
        "        torch.triu(torch.ones(\n",
        "            context_length,             # 1024\n",
        "            context_length,             # 1024\n",
        "          ), diagonal=1)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, num_tokens, embedding_length = x.shape\n",
        "    keys = self.W_key(x)\n",
        "    queries = self.W_query(x)\n",
        "    values = self.W_value(x)\n",
        "\n",
        "    # Add the num_heads and head_dim dimensions\n",
        "    keys = keys.view(batch_size, num_tokens, self.num_heads, self.head_dim)       # Transform to a tensor of dimensions: 2 x 1024 x 12 x 64\n",
        "    queries = queries.view(batch_size, num_tokens, self.num_heads, self.head_dim) # Transform to a tensor of dimensions: 2 x 1024 x 12 x 64\n",
        "    values = values.view(batch_size, num_tokens, self.num_heads, self.head_dim)   # Transform to a tensor of dimensions: 2 x 1024 x 12 x 64\n",
        "\n",
        "    # Transpose from (batch_size, num_tokens, num_heads, head_dim) to (batch_size, num_heads, num_tokens, head_dim)\n",
        "    queries = queries.transpose(1, 2)\n",
        "    keys = keys.transpose(1, 2)\n",
        "    values = values.transpose(1, 2)\n",
        "\n",
        "    # Calculate attention scores\n",
        "    attention_scores = queries @ keys.transpose(2, 3)\n",
        "    mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "    # Mask the attention scores\n",
        "    attention_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "    # Calculate attention weights\n",
        "    attention_weights = torch.softmax(attention_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "\n",
        "    # Apply dropout to attention weights\n",
        "    attention_weights = self.dropout(attention_weights)\n",
        "\n",
        "    # Calculate context vectors\n",
        "    context_vectors = (attention_weights @ values).transpose(1, 2)\n",
        "\n",
        "    # Concatenate the context vectors\n",
        "    context_vectors = context_vectors.contiguous().view(batch_size, num_tokens, self.d_out)\n",
        "    return self.out_proj(context_vectors)"
      ],
      "metadata": {
        "id": "DxJyX7s7FdN5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.att = MultiHeadAttention(\n",
        "        d_in=config[\"emb_dim\"],\n",
        "        d_out=config[\"emb_dim\"],\n",
        "        context_length=config[\"context_length\"],\n",
        "        dropout=config[\"drop_rate\"],\n",
        "        num_heads=config[\"n_heads\"],\n",
        "        qkv_bias=config[\"qkv_bias\"]\n",
        "    )\n",
        "\n",
        "    self.ff = FeedForward(config)\n",
        "    self.norm1 = LayerNorm(config[\"emb_dim\"])\n",
        "    self.norm2 = LayerNorm(config[\"emb_dim\"])\n",
        "    self.drop_shortcut = nn.Dropout(config[\"drop_rate\"])\n",
        "\n",
        "  def forward(self, x):\n",
        "    shortcut = x\n",
        "\n",
        "    # Attention layer\n",
        "    x = self.norm1(x)\n",
        "    x = self.att(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + shortcut         # Add the original input back\n",
        "\n",
        "    # Feedforward layer\n",
        "    shortcut = x\n",
        "    x = self.norm2(x)\n",
        "    x = self.ff(x)\n",
        "    x = self.drop_shortcut(x)\n",
        "    x = x + shortcut         # Add the original input back\n",
        "    return x"
      ],
      "metadata": {
        "id": "PXM2PqkAFe5-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModel(nn.Module):\n",
        "  def __init__(self, config):\n",
        "    super().__init__()\n",
        "\n",
        "    self.tok_emb = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"])\n",
        "    self.pos_emb = nn.Embedding(config[\"context_length\"], config[\"emb_dim\"])\n",
        "    self.drop_embedding = nn.Dropout(config[\"drop_rate\"])\n",
        "\n",
        "    self.trf_blocks = nn.Sequential(\n",
        "        *[TransformerBlock(config) for _ in range(config[\"n_layers\"])]\n",
        "    )\n",
        "\n",
        "    self.final_norm = LayerNorm(config[\"emb_dim\"])\n",
        "    self.out_head = nn.Linear(config[\"emb_dim\"], config[\"vocab_size\"], bias=False)\n",
        "\n",
        "  def forward(self, in_idx):\n",
        "    batch_size, sequence_length = in_idx.shape\n",
        "    tok_emb = self.tok_emb(in_idx)\n",
        "    pos_emb = self.pos_emb(\n",
        "        torch.arange(sequence_length, device=in_idx.device)\n",
        "    )\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.drop_embedding(x)\n",
        "\n",
        "    x = self.trf_blocks(x)\n",
        "    x = self.final_norm(x)\n",
        "    logits = self.out_head(x)\n",
        "    return logits"
      ],
      "metadata": {
        "id": "Kl3Tek4bFgXV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, \"\n",
        "                          \"Right: {right.shape}\"\n",
        "        )\n",
        "    return torch.nn.Parameter(torch.tensor(right))\n",
        "\n",
        "def load_weights_into_gpt(gpt, params):\n",
        "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
        "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
        "\n",
        "    for b in range(len(params[\"blocks\"])):\n",
        "        q_w, k_w, v_w = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
        "        gpt.trf_blocks[b].att.W_key.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
        "        gpt.trf_blocks[b].att.W_value.weight = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
        "\n",
        "        q_b, k_b, v_b = np.split(\n",
        "            (params[\"blocks\"][b][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
        "        gpt.trf_blocks[b].att.W_query.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
        "        gpt.trf_blocks[b].att.W_key.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
        "        gpt.trf_blocks[b].att.W_value.bias = assign(\n",
        "            gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
        "\n",
        "        gpt.trf_blocks[b].att.out_proj.weight = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.weight,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].att.out_proj.bias = assign(\n",
        "            gpt.trf_blocks[b].att.out_proj.bias,\n",
        "            params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].ff.layers[0].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[0].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[0].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
        "        gpt.trf_blocks[b].ff.layers[2].weight = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].weight,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
        "        gpt.trf_blocks[b].ff.layers[2].bias = assign(\n",
        "            gpt.trf_blocks[b].ff.layers[2].bias,\n",
        "            params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
        "\n",
        "        gpt.trf_blocks[b].norm1.scale = assign(\n",
        "            gpt.trf_blocks[b].norm1.scale,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm1.shift = assign(\n",
        "            gpt.trf_blocks[b].norm1.shift,\n",
        "            params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
        "        gpt.trf_blocks[b].norm2.scale = assign(\n",
        "            gpt.trf_blocks[b].norm2.scale,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
        "        gpt.trf_blocks[b].norm2.shift = assign(\n",
        "            gpt.trf_blocks[b].norm2.shift,\n",
        "            params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
        "\n",
        "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
        "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
        "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"])"
      ],
      "metadata": {
        "id": "xu_Jv-HAFpaM"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Load pretrained LLM"
      ],
      "metadata": {
        "id": "U71W-Pgatxn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gpt_download import download_and_load_gpt2\n",
        "\n",
        "\n",
        "BASE_CONFIG = {\n",
        "    \"vocab_size\": 50257,     # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"drop_rate\": 0.0,        # Dropout rate\n",
        "    \"qkv_bias\": True         # Query-key-value bias\n",
        "}\n",
        "\n",
        "model_configs = {\n",
        "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
        "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
        "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
        "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
        "}\n",
        "\n",
        "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
        "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
        "\n",
        "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
        "\n",
        "settings, params = download_and_load_gpt2(\n",
        "    model_size=model_size,\n",
        "    models_dir=\"gpt2\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2N3KD0dExwI",
        "outputId": "259de0e2-0226-42eb-a424-609d98d8ac36"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "checkpoint: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 77.0/77.0 [00:00<00:00, 70.9kiB/s]\n",
            "encoder.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.04M/1.04M [00:00<00:00, 2.78MiB/s]\n",
            "hparams.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 91.0/91.0 [00:00<00:00, 81.3kiB/s]\n",
            "model.ckpt.data-00000-of-00001: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.42G/1.42G [00:54<00:00, 26.1MiB/s]\n",
            "model.ckpt.index: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10.4k/10.4k [00:00<00:00, 7.43MiB/s]\n",
            "model.ckpt.meta: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 927k/927k [00:00<00:00, 2.18MiB/s]\n",
            "vocab.bpe: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:00<00:00, 1.38MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTModel(BASE_CONFIG)\n",
        "load_weights_into_gpt(model, params)\n",
        "model.eval();"
      ],
      "metadata": {
        "id": "s39F1wLoGua5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Generate some sample text"
      ],
      "metadata": {
        "id": "OhmGZoAJGPEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, idx, max_new_tokens, context_size,\n",
        "             temperature=0.0, top_k=None, eos_id=None):\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "        if top_k is not None:\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(\n",
        "                logits < min_val,\n",
        "                torch.tensor(float('-inf')).to(logits.device),\n",
        "                logits\n",
        "            )\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "        if idx_next == eos_id:\n",
        "            break\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "    return idx\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0)\n",
        "    return tokenizer.decode(flat.tolist())"
      ],
      "metadata": {
        "id": "RoZTwjSTGSyZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "input_text = format_input(val_data[0])\n",
        "print(input_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzOlfMyPF0RN",
        "outputId": "f087b575-cd6f-4d24-f88f-3b075bd9f2fb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Answer the following question.\n",
            "\n",
            "### Input:\n",
            "How much cash and cash equivalents did the company have at the end of 2023, and how does this compare to the end of 2022?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(input_text, tokenizer),\n",
        "    max_new_tokens=35,\n",
        "    context_size=BASE_CONFIG[\"context_length\"],\n",
        "    eos_id=50256,\n",
        ")\n",
        "generated_text = token_ids_to_text(token_ids, tokenizer)"
      ],
      "metadata": {
        "id": "Jh0PbFJXGByQ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_text = generated_text[len(input_text):].strip()\n",
        "print(response_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JX6nb8BJGhG-",
        "outputId": "fb2fee6e-1717-4f89-8348-f330743fcbd6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Output:\n",
            "\n",
            "How much cash and cash equivalents did the company have at the end of 2023, and how does this compare to the end of 2022?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "input_text = format_input(val_data[0])\n",
        "print(input_text)\n",
        "print()\n",
        "print(\"### Output:\")\n",
        "print(response_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJeUcBiAWLR-",
        "outputId": "8645f645-ab00-4a6c-c364-0b73e5a51869"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Answer the following question.\n",
            "\n",
            "### Input:\n",
            "How much cash and cash equivalents did the company have at the end of 2023, and how does this compare to the end of 2022?\n",
            "\n",
            "### Output:\n",
            "### Output:\n",
            "\n",
            "How much cash and cash equivalents did the company have at the end of 2023, and how does this compare to the end of 2022?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Finetune the LLM"
      ],
      "metadata": {
        "id": "4gw-i9_gH_LR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch = input_batch.to(device)\n",
        "    target_batch = target_batch.to(device)\n",
        "    model.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(\n",
        "        logits.flatten(0, 1), target_batch.flatten()\n",
        "    )\n",
        "    return loss"
      ],
      "metadata": {
        "id": "9pyFG6JJPcCb"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(\n",
        "                input_batch, target_batch, model, device\n",
        "            )\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches"
      ],
      "metadata": {
        "id": "IoHZ02F8-2uz"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(\n",
        "            train_loader, model, device, num_batches=eval_iter\n",
        "        )\n",
        "        val_loss = calc_loss_loader(\n",
        "            val_loader, model, device, num_batches=eval_iter\n",
        "        )\n",
        "    model.train()\n",
        "    return train_loss, val_loss"
      ],
      "metadata": {
        "id": "lpIUVnxfPfHe"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_simple(model, idx,\n",
        "                         max_new_tokens, context_size):\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        logits = logits[:, -1, :]\n",
        "        probas = torch.softmax(logits, dim=-1)\n",
        "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \"))\n",
        "    model.train()"
      ],
      "metadata": {
        "id": "TZRpdZVbPiBQ"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_simple(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    optimizer,\n",
        "    device,\n",
        "    num_epochs,\n",
        "    eval_freq,\n",
        "    eval_iter,\n",
        "    start_context,\n",
        "    tokenizer,\n",
        "):\n",
        "  train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "  tokens_seen, global_step = 0, -1\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "      model.train()\n",
        "      for input_batch, target_batch in train_loader:\n",
        "          optimizer.zero_grad()\n",
        "          loss = calc_loss_batch(\n",
        "              input_batch, target_batch, model, device\n",
        "          )\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          tokens_seen += input_batch.numel()\n",
        "          global_step += 1\n",
        "\n",
        "          if global_step % eval_freq == 0:\n",
        "              train_loss, val_loss = evaluate_model(\n",
        "                  model, train_loader, val_loader, device, eval_iter)\n",
        "              train_losses.append(train_loss)\n",
        "              val_losses.append(val_loss)\n",
        "              track_tokens_seen.append(tokens_seen)\n",
        "              print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                    f\"Train loss {train_loss:.3f}, \"\n",
        "                    f\"Val loss {val_loss:.3f}\"\n",
        "              )\n",
        "\n",
        "      generate_and_print_sample(\n",
        "          model, tokenizer, device, start_context\n",
        "      )\n",
        "  return train_losses, val_losses, track_tokens_seen"
      ],
      "metadata": {
        "id": "pLJ43cwa--JM"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "torch.manual_seed(123)\n",
        "\n",
        "with torch.no_grad():\n",
        "    train_loss = calc_loss_loader(\n",
        "        train_loader, model, device, num_batches=5\n",
        "    )\n",
        "    val_loss = calc_loss_loader(\n",
        "        val_loader, model, device, num_batches=5\n",
        ")\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUUtThpN_TH4",
        "outputId": "1ffc60a5-ad60-40c9-f3c8-9148ed73be15"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 3.6915621757507324\n",
            "Validation loss: 3.8882946968078613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "torch.manual_seed(123)\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(), lr=0.00005, weight_decay=0.1\n",
        ")\n",
        "num_epochs = 2\n",
        "\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=format_input(val_data[0]), tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
      ],
      "metadata": {
        "id": "GxMOtF8f_VHh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91d43c02-1c7f-4dea-e4df-b25f22465287"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 2.683, Val loss 2.721\n",
            "Ep 1 (Step 000005): Train loss 1.336, Val loss 1.216\n",
            "Ep 1 (Step 000010): Train loss 1.316, Val loss 1.082\n",
            "Ep 1 (Step 000015): Train loss 1.311, Val loss 1.073\n",
            "Ep 1 (Step 000020): Train loss 1.333, Val loss 1.051\n",
            "Ep 1 (Step 000025): Train loss 1.284, Val loss 1.034\n",
            "Ep 1 (Step 000030): Train loss 1.141, Val loss 1.039\n",
            "Ep 1 (Step 000035): Train loss 1.178, Val loss 1.027\n",
            "Ep 1 (Step 000040): Train loss 1.092, Val loss 1.024\n",
            "Ep 1 (Step 000045): Train loss 1.195, Val loss 1.010\n",
            "Ep 1 (Step 000050): Train loss 1.213, Val loss 1.013\n",
            "Ep 1 (Step 000055): Train loss 1.323, Val loss 1.005\n",
            "Ep 1 (Step 000060): Train loss 1.323, Val loss 1.007\n",
            "Ep 1 (Step 000065): Train loss 1.247, Val loss 1.009\n",
            "Ep 1 (Step 000070): Train loss 1.084, Val loss 0.996\n",
            "Ep 1 (Step 000075): Train loss 1.274, Val loss 0.986\n",
            "Ep 1 (Step 000080): Train loss 1.185, Val loss 0.983\n",
            "Ep 1 (Step 000085): Train loss 1.175, Val loss 0.987\n",
            "Ep 1 (Step 000090): Train loss 1.169, Val loss 0.993\n",
            "Ep 1 (Step 000095): Train loss 1.047, Val loss 0.991\n",
            "Ep 1 (Step 000100): Train loss 1.121, Val loss 0.985\n",
            "Ep 1 (Step 000105): Train loss 1.317, Val loss 0.977\n",
            "Ep 1 (Step 000110): Train loss 1.145, Val loss 0.979\n",
            "Ep 1 (Step 000115): Train loss 1.069, Val loss 0.977\n",
            "Ep 1 (Step 000120): Train loss 1.135, Val loss 0.975\n",
            "Ep 1 (Step 000125): Train loss 1.125, Val loss 0.971\n",
            "Ep 1 (Step 000130): Train loss 1.251, Val loss 0.969\n",
            "Ep 1 (Step 000135): Train loss 1.143, Val loss 0.976\n",
            "Ep 1 (Step 000140): Train loss 1.153, Val loss 0.969\n",
            "Ep 1 (Step 000145): Train loss 1.111, Val loss 0.960\n",
            "Ep 1 (Step 000150): Train loss 1.072, Val loss 0.957\n",
            "Ep 1 (Step 000155): Train loss 1.005, Val loss 0.957\n",
            "Ep 1 (Step 000160): Train loss 1.080, Val loss 0.956\n",
            "Ep 1 (Step 000165): Train loss 1.095, Val loss 0.965\n",
            "Ep 1 (Step 000170): Train loss 1.128, Val loss 0.958\n",
            "Ep 1 (Step 000175): Train loss 1.064, Val loss 0.954\n",
            "Ep 1 (Step 000180): Train loss 1.109, Val loss 0.958\n",
            "Ep 1 (Step 000185): Train loss 1.045, Val loss 0.963\n",
            "Ep 1 (Step 000190): Train loss 1.143, Val loss 0.954\n",
            "Ep 1 (Step 000195): Train loss 1.047, Val loss 0.953\n",
            "Ep 1 (Step 000200): Train loss 1.116, Val loss 0.958\n",
            "Ep 1 (Step 000205): Train loss 1.154, Val loss 0.952\n",
            "Ep 1 (Step 000210): Train loss 1.213, Val loss 0.952\n",
            "Ep 1 (Step 000215): Train loss 1.116, Val loss 0.944\n",
            "Ep 1 (Step 000220): Train loss 1.069, Val loss 0.943\n",
            "Ep 1 (Step 000225): Train loss 1.046, Val loss 0.948\n",
            "Ep 1 (Step 000230): Train loss 1.245, Val loss 0.949\n",
            "Ep 1 (Step 000235): Train loss 1.055, Val loss 0.949\n",
            "Ep 1 (Step 000240): Train loss 1.064, Val loss 0.954\n",
            "Ep 1 (Step 000245): Train loss 1.120, Val loss 0.954\n",
            "Ep 1 (Step 000250): Train loss 1.083, Val loss 0.961\n",
            "Ep 1 (Step 000255): Train loss 1.030, Val loss 0.956\n",
            "Ep 1 (Step 000260): Train loss 1.027, Val loss 0.949\n",
            "Ep 1 (Step 000265): Train loss 1.083, Val loss 0.947\n",
            "Ep 1 (Step 000270): Train loss 1.016, Val loss 0.940\n",
            "Ep 1 (Step 000275): Train loss 1.105, Val loss 0.941\n",
            "Ep 1 (Step 000280): Train loss 1.107, Val loss 0.939\n",
            "Ep 1 (Step 000285): Train loss 1.086, Val loss 0.932\n",
            "Ep 1 (Step 000290): Train loss 1.056, Val loss 0.939\n",
            "Ep 1 (Step 000295): Train loss 1.050, Val loss 0.939\n",
            "Ep 1 (Step 000300): Train loss 0.887, Val loss 0.942\n",
            "Ep 1 (Step 000305): Train loss 0.954, Val loss 0.939\n",
            "Ep 1 (Step 000310): Train loss 1.000, Val loss 0.930\n",
            "Ep 1 (Step 000315): Train loss 1.100, Val loss 0.924\n",
            "Ep 1 (Step 000320): Train loss 0.894, Val loss 0.931\n",
            "Ep 1 (Step 000325): Train loss 0.958, Val loss 0.938\n",
            "Ep 1 (Step 000330): Train loss 0.967, Val loss 0.950\n",
            "Ep 1 (Step 000335): Train loss 1.114, Val loss 0.953\n",
            "Ep 1 (Step 000340): Train loss 1.110, Val loss 0.943\n",
            "Ep 1 (Step 000345): Train loss 1.033, Val loss 0.931\n",
            "Ep 1 (Step 000350): Train loss 1.065, Val loss 0.928\n",
            "Ep 1 (Step 000355): Train loss 1.072, Val loss 0.930\n",
            "Ep 1 (Step 000360): Train loss 0.980, Val loss 0.932\n",
            "Ep 1 (Step 000365): Train loss 0.991, Val loss 0.935\n",
            "Ep 1 (Step 000370): Train loss 1.075, Val loss 0.941\n",
            "Ep 1 (Step 000375): Train loss 1.048, Val loss 0.935\n",
            "Ep 1 (Step 000380): Train loss 1.011, Val loss 0.928\n",
            "Ep 1 (Step 000385): Train loss 0.997, Val loss 0.929\n",
            "Ep 1 (Step 000390): Train loss 1.111, Val loss 0.931\n",
            "Ep 1 (Step 000395): Train loss 1.000, Val loss 0.931\n",
            "Ep 1 (Step 000400): Train loss 1.037, Val loss 0.930\n",
            "Ep 1 (Step 000405): Train loss 0.912, Val loss 0.926\n",
            "Ep 1 (Step 000410): Train loss 1.083, Val loss 0.931\n",
            "Ep 1 (Step 000415): Train loss 1.018, Val loss 0.935\n",
            "Ep 1 (Step 000420): Train loss 1.000, Val loss 0.933\n",
            "Ep 1 (Step 000425): Train loss 0.932, Val loss 0.926\n",
            "Ep 1 (Step 000430): Train loss 1.009, Val loss 0.929\n",
            "Ep 1 (Step 000435): Train loss 1.050, Val loss 0.933\n",
            "Ep 1 (Step 000440): Train loss 0.956, Val loss 0.932\n",
            "Ep 1 (Step 000445): Train loss 0.994, Val loss 0.930\n",
            "Ep 1 (Step 000450): Train loss 0.950, Val loss 0.925\n",
            "Ep 1 (Step 000455): Train loss 1.068, Val loss 0.924\n",
            "Ep 1 (Step 000460): Train loss 1.079, Val loss 0.918\n",
            "Ep 1 (Step 000465): Train loss 0.933, Val loss 0.918\n",
            "Ep 1 (Step 000470): Train loss 1.029, Val loss 0.926\n",
            "Ep 1 (Step 000475): Train loss 0.937, Val loss 0.929\n",
            "Ep 1 (Step 000480): Train loss 1.027, Val loss 0.930\n",
            "Ep 1 (Step 000485): Train loss 1.080, Val loss 0.933\n",
            "Ep 1 (Step 000490): Train loss 0.969, Val loss 0.937\n",
            "Ep 1 (Step 000495): Train loss 0.889, Val loss 0.932\n",
            "Ep 1 (Step 000500): Train loss 0.948, Val loss 0.929\n",
            "Ep 1 (Step 000505): Train loss 0.896, Val loss 0.925\n",
            "Ep 1 (Step 000510): Train loss 0.998, Val loss 0.930\n",
            "Ep 1 (Step 000515): Train loss 0.920, Val loss 0.930\n",
            "Ep 1 (Step 000520): Train loss 0.846, Val loss 0.927\n",
            "Ep 1 (Step 000525): Train loss 0.946, Val loss 0.926\n",
            "Ep 1 (Step 000530): Train loss 0.933, Val loss 0.926\n",
            "Ep 1 (Step 000535): Train loss 0.910, Val loss 0.923\n",
            "Ep 1 (Step 000540): Train loss 0.923, Val loss 0.926\n",
            "Ep 1 (Step 000545): Train loss 0.983, Val loss 0.926\n",
            "Ep 1 (Step 000550): Train loss 0.934, Val loss 0.920\n",
            "Ep 1 (Step 000555): Train loss 0.849, Val loss 0.919\n",
            "Ep 1 (Step 000560): Train loss 0.939, Val loss 0.919\n",
            "Ep 1 (Step 000565): Train loss 0.879, Val loss 0.920\n",
            "Ep 1 (Step 000570): Train loss 1.048, Val loss 0.923\n",
            "Ep 1 (Step 000575): Train loss 0.995, Val loss 0.924\n",
            "Ep 1 (Step 000580): Train loss 1.063, Val loss 0.922\n",
            "Ep 1 (Step 000585): Train loss 0.933, Val loss 0.920\n",
            "Ep 1 (Step 000590): Train loss 0.870, Val loss 0.916\n",
            "Ep 1 (Step 000595): Train loss 0.897, Val loss 0.918\n",
            "Ep 1 (Step 000600): Train loss 0.997, Val loss 0.918\n",
            "Ep 1 (Step 000605): Train loss 0.948, Val loss 0.919\n",
            "Ep 1 (Step 000610): Train loss 0.971, Val loss 0.918\n",
            "Ep 1 (Step 000615): Train loss 0.936, Val loss 0.920\n",
            "Ep 1 (Step 000620): Train loss 0.834, Val loss 0.920\n",
            "Ep 1 (Step 000625): Train loss 0.849, Val loss 0.921\n",
            "Ep 1 (Step 000630): Train loss 0.949, Val loss 0.925\n",
            "Ep 1 (Step 000635): Train loss 0.837, Val loss 0.926\n",
            "Ep 1 (Step 000640): Train loss 0.817, Val loss 0.926\n",
            "Ep 1 (Step 000645): Train loss 0.876, Val loss 0.924\n",
            "Ep 1 (Step 000650): Train loss 1.018, Val loss 0.921\n",
            "Ep 1 (Step 000655): Train loss 1.013, Val loss 0.919\n",
            "Ep 1 (Step 000660): Train loss 0.935, Val loss 0.916\n",
            "Ep 1 (Step 000665): Train loss 0.891, Val loss 0.913\n",
            "Ep 1 (Step 000670): Train loss 0.881, Val loss 0.915\n",
            "Ep 1 (Step 000675): Train loss 0.964, Val loss 0.915\n",
            "Ep 1 (Step 000680): Train loss 0.819, Val loss 0.919\n",
            "Ep 1 (Step 000685): Train loss 0.845, Val loss 0.923\n",
            "Ep 1 (Step 000690): Train loss 0.848, Val loss 0.922\n",
            "Ep 1 (Step 000695): Train loss 0.852, Val loss 0.921\n",
            "Ep 1 (Step 000700): Train loss 0.876, Val loss 0.924\n",
            "Ep 1 (Step 000705): Train loss 0.928, Val loss 0.926\n",
            "Ep 1 (Step 000710): Train loss 0.839, Val loss 0.922\n",
            "Ep 1 (Step 000715): Train loss 0.857, Val loss 0.917\n",
            "Ep 1 (Step 000720): Train loss 0.801, Val loss 0.917\n",
            "Ep 1 (Step 000725): Train loss 0.866, Val loss 0.916\n",
            "Ep 1 (Step 000730): Train loss 0.940, Val loss 0.917\n",
            "Ep 1 (Step 000735): Train loss 0.884, Val loss 0.918\n",
            "Ep 1 (Step 000740): Train loss 0.915, Val loss 0.917\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Answer the following question.  ### Input: How much cash and cash equivalents did the company have at the end of 2023, and how does this compare to the end of 2022?  ### Response: The company had $1,902 million in cash and cash equivalents at the end of 2023, compared to $1,902 million at the end of 2022.<|endoftext|>The U.S. Supreme Court has\n",
            "Ep 2 (Step 000745): Train loss 0.921, Val loss 0.914\n",
            "Ep 2 (Step 000750): Train loss 0.839, Val loss 0.917\n",
            "Ep 2 (Step 000755): Train loss 0.957, Val loss 0.931\n",
            "Ep 2 (Step 000760): Train loss 0.878, Val loss 0.932\n",
            "Ep 2 (Step 000765): Train loss 0.926, Val loss 0.933\n",
            "Ep 2 (Step 000770): Train loss 0.930, Val loss 0.938\n",
            "Ep 2 (Step 000775): Train loss 0.829, Val loss 0.944\n",
            "Ep 2 (Step 000780): Train loss 0.836, Val loss 0.953\n",
            "Ep 2 (Step 000785): Train loss 0.831, Val loss 0.953\n",
            "Ep 2 (Step 000790): Train loss 0.898, Val loss 0.946\n",
            "Ep 2 (Step 000795): Train loss 0.926, Val loss 0.942\n",
            "Ep 2 (Step 000800): Train loss 0.801, Val loss 0.936\n",
            "Ep 2 (Step 000805): Train loss 0.774, Val loss 0.940\n",
            "Ep 2 (Step 000810): Train loss 0.792, Val loss 0.943\n",
            "Ep 2 (Step 000815): Train loss 0.990, Val loss 0.941\n",
            "Ep 2 (Step 000820): Train loss 0.884, Val loss 0.934\n",
            "Ep 2 (Step 000825): Train loss 0.789, Val loss 0.937\n",
            "Ep 2 (Step 000830): Train loss 0.829, Val loss 0.938\n",
            "Ep 2 (Step 000835): Train loss 0.733, Val loss 0.929\n",
            "Ep 2 (Step 000840): Train loss 0.886, Val loss 0.923\n",
            "Ep 2 (Step 000845): Train loss 0.684, Val loss 0.927\n",
            "Ep 2 (Step 000850): Train loss 0.828, Val loss 0.934\n",
            "Ep 2 (Step 000855): Train loss 0.703, Val loss 0.933\n",
            "Ep 2 (Step 000860): Train loss 0.939, Val loss 0.930\n",
            "Ep 2 (Step 000865): Train loss 0.777, Val loss 0.931\n",
            "Ep 2 (Step 000870): Train loss 0.865, Val loss 0.934\n",
            "Ep 2 (Step 000875): Train loss 0.773, Val loss 0.936\n",
            "Ep 2 (Step 000880): Train loss 0.777, Val loss 0.941\n",
            "Ep 2 (Step 000885): Train loss 0.824, Val loss 0.938\n",
            "Ep 2 (Step 000890): Train loss 0.821, Val loss 0.931\n",
            "Ep 2 (Step 000895): Train loss 0.790, Val loss 0.934\n",
            "Ep 2 (Step 000900): Train loss 0.734, Val loss 0.941\n",
            "Ep 2 (Step 000905): Train loss 0.715, Val loss 0.950\n",
            "Ep 2 (Step 000910): Train loss 0.795, Val loss 0.946\n",
            "Ep 2 (Step 000915): Train loss 0.792, Val loss 0.940\n",
            "Ep 2 (Step 000920): Train loss 0.862, Val loss 0.935\n",
            "Ep 2 (Step 000925): Train loss 0.910, Val loss 0.935\n",
            "Ep 2 (Step 000930): Train loss 0.852, Val loss 0.938\n",
            "Ep 2 (Step 000935): Train loss 0.794, Val loss 0.932\n",
            "Ep 2 (Step 000940): Train loss 0.768, Val loss 0.922\n",
            "Ep 2 (Step 000945): Train loss 0.768, Val loss 0.921\n",
            "Ep 2 (Step 000950): Train loss 0.823, Val loss 0.927\n",
            "Ep 2 (Step 000955): Train loss 0.680, Val loss 0.934\n",
            "Ep 2 (Step 000960): Train loss 0.758, Val loss 0.936\n",
            "Ep 2 (Step 000965): Train loss 0.792, Val loss 0.935\n",
            "Ep 2 (Step 000970): Train loss 0.821, Val loss 0.934\n",
            "Ep 2 (Step 000975): Train loss 0.727, Val loss 0.932\n",
            "Ep 2 (Step 000980): Train loss 0.717, Val loss 0.934\n",
            "Ep 2 (Step 000985): Train loss 0.796, Val loss 0.935\n",
            "Ep 2 (Step 000990): Train loss 0.688, Val loss 0.937\n",
            "Ep 2 (Step 000995): Train loss 0.831, Val loss 0.936\n",
            "Ep 2 (Step 001000): Train loss 0.751, Val loss 0.933\n",
            "Ep 2 (Step 001005): Train loss 0.815, Val loss 0.934\n",
            "Ep 2 (Step 001010): Train loss 0.780, Val loss 0.935\n",
            "Ep 2 (Step 001015): Train loss 0.805, Val loss 0.934\n",
            "Ep 2 (Step 001020): Train loss 0.809, Val loss 0.932\n",
            "Ep 2 (Step 001025): Train loss 0.702, Val loss 0.930\n",
            "Ep 2 (Step 001030): Train loss 0.832, Val loss 0.932\n",
            "Ep 2 (Step 001035): Train loss 0.751, Val loss 0.936\n",
            "Ep 2 (Step 001040): Train loss 0.787, Val loss 0.926\n",
            "Ep 2 (Step 001045): Train loss 0.746, Val loss 0.918\n",
            "Ep 2 (Step 001050): Train loss 0.808, Val loss 0.915\n",
            "Ep 2 (Step 001055): Train loss 0.827, Val loss 0.915\n",
            "Ep 2 (Step 001060): Train loss 0.646, Val loss 0.919\n",
            "Ep 2 (Step 001065): Train loss 0.706, Val loss 0.926\n",
            "Ep 2 (Step 001070): Train loss 0.773, Val loss 0.925\n",
            "Ep 2 (Step 001075): Train loss 0.733, Val loss 0.921\n",
            "Ep 2 (Step 001080): Train loss 0.772, Val loss 0.915\n",
            "Ep 2 (Step 001085): Train loss 0.658, Val loss 0.911\n",
            "Ep 2 (Step 001090): Train loss 0.730, Val loss 0.906\n",
            "Ep 2 (Step 001095): Train loss 0.865, Val loss 0.903\n",
            "Ep 2 (Step 001100): Train loss 0.704, Val loss 0.909\n",
            "Ep 2 (Step 001105): Train loss 0.704, Val loss 0.911\n",
            "Ep 2 (Step 001110): Train loss 0.717, Val loss 0.912\n",
            "Ep 2 (Step 001115): Train loss 0.788, Val loss 0.908\n",
            "Ep 2 (Step 001120): Train loss 0.739, Val loss 0.904\n",
            "Ep 2 (Step 001125): Train loss 0.850, Val loss 0.902\n",
            "Ep 2 (Step 001130): Train loss 0.789, Val loss 0.904\n",
            "Ep 2 (Step 001135): Train loss 0.792, Val loss 0.900\n",
            "Ep 2 (Step 001140): Train loss 0.679, Val loss 0.901\n",
            "Ep 2 (Step 001145): Train loss 0.789, Val loss 0.908\n",
            "Ep 2 (Step 001150): Train loss 0.771, Val loss 0.913\n",
            "Ep 2 (Step 001155): Train loss 0.773, Val loss 0.910\n",
            "Ep 2 (Step 001160): Train loss 0.734, Val loss 0.909\n",
            "Ep 2 (Step 001165): Train loss 0.803, Val loss 0.908\n",
            "Ep 2 (Step 001170): Train loss 0.725, Val loss 0.908\n",
            "Ep 2 (Step 001175): Train loss 0.637, Val loss 0.907\n",
            "Ep 2 (Step 001180): Train loss 0.729, Val loss 0.905\n",
            "Ep 2 (Step 001185): Train loss 0.812, Val loss 0.902\n",
            "Ep 2 (Step 001190): Train loss 0.672, Val loss 0.903\n",
            "Ep 2 (Step 001195): Train loss 0.685, Val loss 0.908\n",
            "Ep 2 (Step 001200): Train loss 0.720, Val loss 0.910\n",
            "Ep 2 (Step 001205): Train loss 0.710, Val loss 0.910\n",
            "Ep 2 (Step 001210): Train loss 0.655, Val loss 0.914\n",
            "Ep 2 (Step 001215): Train loss 0.732, Val loss 0.920\n",
            "Ep 2 (Step 001220): Train loss 0.687, Val loss 0.925\n",
            "Ep 2 (Step 001225): Train loss 0.789, Val loss 0.924\n",
            "Ep 2 (Step 001230): Train loss 0.791, Val loss 0.920\n",
            "Ep 2 (Step 001235): Train loss 0.709, Val loss 0.925\n",
            "Ep 2 (Step 001240): Train loss 0.717, Val loss 0.929\n",
            "Ep 2 (Step 001245): Train loss 0.693, Val loss 0.922\n",
            "Ep 2 (Step 001250): Train loss 0.705, Val loss 0.922\n",
            "Ep 2 (Step 001255): Train loss 0.670, Val loss 0.924\n",
            "Ep 2 (Step 001260): Train loss 0.644, Val loss 0.922\n",
            "Ep 2 (Step 001265): Train loss 0.749, Val loss 0.920\n",
            "Ep 2 (Step 001270): Train loss 0.632, Val loss 0.921\n",
            "Ep 2 (Step 001275): Train loss 0.761, Val loss 0.917\n",
            "Ep 2 (Step 001280): Train loss 0.680, Val loss 0.912\n",
            "Ep 2 (Step 001285): Train loss 0.704, Val loss 0.916\n",
            "Ep 2 (Step 001290): Train loss 0.681, Val loss 0.916\n",
            "Ep 2 (Step 001295): Train loss 0.637, Val loss 0.908\n",
            "Ep 2 (Step 001300): Train loss 0.716, Val loss 0.907\n",
            "Ep 2 (Step 001305): Train loss 0.682, Val loss 0.909\n",
            "Ep 2 (Step 001310): Train loss 0.655, Val loss 0.909\n",
            "Ep 2 (Step 001315): Train loss 0.685, Val loss 0.909\n",
            "Ep 2 (Step 001320): Train loss 0.730, Val loss 0.910\n",
            "Ep 2 (Step 001325): Train loss 0.602, Val loss 0.915\n",
            "Ep 2 (Step 001330): Train loss 0.637, Val loss 0.913\n",
            "Ep 2 (Step 001335): Train loss 0.677, Val loss 0.915\n",
            "Ep 2 (Step 001340): Train loss 0.663, Val loss 0.912\n",
            "Ep 2 (Step 001345): Train loss 0.748, Val loss 0.908\n",
            "Ep 2 (Step 001350): Train loss 0.783, Val loss 0.905\n",
            "Ep 2 (Step 001355): Train loss 0.762, Val loss 0.901\n",
            "Ep 2 (Step 001360): Train loss 0.721, Val loss 0.902\n",
            "Ep 2 (Step 001365): Train loss 0.678, Val loss 0.908\n",
            "Ep 2 (Step 001370): Train loss 0.578, Val loss 0.914\n",
            "Ep 2 (Step 001375): Train loss 0.705, Val loss 0.917\n",
            "Ep 2 (Step 001380): Train loss 0.667, Val loss 0.914\n",
            "Ep 2 (Step 001385): Train loss 0.618, Val loss 0.911\n",
            "Ep 2 (Step 001390): Train loss 0.729, Val loss 0.911\n",
            "Ep 2 (Step 001395): Train loss 0.677, Val loss 0.908\n",
            "Ep 2 (Step 001400): Train loss 0.718, Val loss 0.905\n",
            "Ep 2 (Step 001405): Train loss 0.714, Val loss 0.907\n",
            "Ep 2 (Step 001410): Train loss 0.564, Val loss 0.909\n",
            "Ep 2 (Step 001415): Train loss 0.672, Val loss 0.914\n",
            "Ep 2 (Step 001420): Train loss 0.639, Val loss 0.914\n",
            "Ep 2 (Step 001425): Train loss 0.742, Val loss 0.912\n",
            "Ep 2 (Step 001430): Train loss 0.648, Val loss 0.906\n",
            "Ep 2 (Step 001435): Train loss 0.626, Val loss 0.902\n",
            "Ep 2 (Step 001440): Train loss 0.609, Val loss 0.900\n",
            "Ep 2 (Step 001445): Train loss 0.677, Val loss 0.900\n",
            "Ep 2 (Step 001450): Train loss 0.628, Val loss 0.903\n",
            "Ep 2 (Step 001455): Train loss 0.650, Val loss 0.905\n",
            "Ep 2 (Step 001460): Train loss 0.626, Val loss 0.904\n",
            "Ep 2 (Step 001465): Train loss 0.648, Val loss 0.903\n",
            "Ep 2 (Step 001470): Train loss 0.671, Val loss 0.910\n",
            "Ep 2 (Step 001475): Train loss 0.700, Val loss 0.909\n",
            "Ep 2 (Step 001480): Train loss 0.679, Val loss 0.907\n",
            "Ep 2 (Step 001485): Train loss 0.625, Val loss 0.904\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Answer the following question.  ### Input: How much cash and cash equivalents did the company have at the end of 2023, and how does this compare to the end of 2022?  ### Response: At the end of 2023, the company had cash and cash equivalents of $1,859.8 million, compared to $1,859.6 million at the end of 2022.<|endoftext|>The company's\n",
            "Training completed in 29.32 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(\n",
        "        epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\"\n",
        "    )\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "    ax2 = ax1.twiny()\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "07ef4Oc2_a6Q",
        "outputId": "c62b2aae-b5ce-4de9-a78f-9e44a9491871"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Save LLM responses"
      ],
      "metadata": {
        "id": "brOhxwRF_m3Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "for entry in test_data[:3]:\n",
        "    input_text = format_input(entry)\n",
        "    token_ids = generate(\n",
        "        model=model,\n",
        "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "        max_new_tokens=256,\n",
        "        context_size=BASE_CONFIG[\"context_length\"],\n",
        "        eos_id=50256\n",
        "    )\n",
        "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "\n",
        "    response_text = (\n",
        "        generated_text[len(input_text):]\n",
        "        .replace(\"### Response:\", \"\")\n",
        "        .strip()\n",
        "    )\n",
        "    print(input_text)\n",
        "    print()\n",
        "    print(f\"### Output:\\n{response_text.strip()}\")\n",
        "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
        "    print()\n",
        "    print()\n",
        "    print(\"-------------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "035vK0yR_oNa",
        "outputId": "8002c85a-c563-4977-a95f-bbfbd348cd5a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Answer the following question.\n",
            "\n",
            "### Input:\n",
            "How many countries and territories did Hilton's development pipeline encompass as of the end of 2023?\n",
            "\n",
            "### Output:\n",
            "As of the end of 2023, Hilton's development pipeline encompassed over 220 countries and territories.\n",
            "\n",
            "Correct response:\n",
            ">> 118\n",
            "\n",
            "\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Answer the following question.\n",
            "\n",
            "### Input:\n",
            "How does the company manage fluctuations in foreign currency exchange rates?\n",
            "\n",
            "### Output:\n",
            "The company manages fluctuations in foreign currency exchange rates by hedging against them by purchasing U.S. dollar denominated debt securities and by purchasing U.S. dollar-denominated equity securities.\n",
            "\n",
            "Correct response:\n",
            ">> The company hedges foreign currency exchange-based cash flow variability of certain fees using forward contracts designated as hedging instruments. Additionally, it holds short-term forward contracts to offset exposure to fluctuations in its foreign currency denominated cash balances and intercompany financing arrangements, without designating these forward contracts as hedging instruments.\n",
            "\n",
            "\n",
            "-------------------------------------\n",
            "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Answer the following question.\n",
            "\n",
            "### Input:\n",
            "What factors contribute to the seasonality of the hospitality industry?\n",
            "\n",
            "### Output:\n",
            "The seasonality of the hospitality industry is influenced by factors such as holiday demand, seasonality of travel, and the number of hotels and resorts.\n",
            "\n",
            "Correct response:\n",
            ">> The seasonality of the hospitality industry is influenced by variations in the levels of demand experienced at different properties, depending on their location, type of property, and the competitive mix within the specific location.\n",
            "\n",
            "\n",
            "-------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
        "    input_text = format_input(entry)\n",
        "\n",
        "    token_ids = generate(\n",
        "        model=model,\n",
        "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
        "        max_new_tokens=256,\n",
        "        context_size=BASE_CONFIG[\"context_length\"],\n",
        "        eos_id=50256\n",
        "    )\n",
        "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
        "\n",
        "    response_text = (\n",
        "        generated_text[len(input_text):]\n",
        "        .replace(\"### Response:\", \"\")\n",
        "        .strip()\n",
        "    )\n",
        "    test_data[i][\"model_response\"] = response_text\n",
        "\n",
        "with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
        "    json.dump(test_data, file, indent=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkgNjm0X_sGT",
        "outputId": "2e39d583-db27-4592-f005-b1679a203125"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 700/700 [11:38<00:00,  1.00it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lam9ssEw_uHq",
        "outputId": "c4fde100-d61b-4118-bb54-2131bbb553e1"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'instruction': 'Answer the following question.', 'input': \"How many countries and territories did Hilton's development pipeline encompass as of the end of 2023?\", 'output': '118', 'model_response': \"As of the end of 2023, Hilton's development pipeline encompassed over 220 countries and territories.\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Save the model\n",
        "file_name = f\"{re.sub(r'[ ()]', '', CHOOSE_MODEL) }-sft.pth\"\n",
        "torch.save(model.state_dict(), file_name)\n",
        "print(f\"Model saved as {file_name}\")\n",
        "\n",
        "# Load the model (when needed)\n",
        "# model.load_state_dict(torch.load(\"gpt2 -medium355M-sft.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYt4PapF_ver",
        "outputId": "f624793c-323f-4b55-eb0f-87d93cdc6c1f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as gpt2-medium355M-sft.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w4rOBKpPRoFC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}